---
title: Polytomous Logistic Regression Model to Predict Region in County Crime Dataset
author: "Amir Idris"
output:
  pdf_document: default
  html_notebook: default
---
```{r}
if (!require("dplyr")){
  install.packages("dplyr")
}

if (!require("nnet")){
  install.packages("nnet")
}

if (!require("formattable")){
  install.packages("formattable")
}

library(dplyr)
library(nnet)
library(formattable)
```


First, let's load the data.

```{r}
crime <- read.table("../data/APPENC02.txt", sep="", header=FALSE)
UNI <- 2244
set.seed(UNI)
index <- sample(c(1:440))
crime <- crime[index[1:250],]

# Rename column names to reflect meaning
column.names <- c("Id", "County Name", "State", "Land Area", "Total Pop.", "Percent of Pop. Age 18-34", "Percent of Pop. Age >= 65", "Num of Active Physicians", "Num of Hospital Beds", "Total Serious Crimes", "Percent High School Graduates", "Percent Bachelor's Degree", "Percent Below Poverty Level", "Percent Unemployment", "Per Capita Income", "Total personal income", "Geographic Region")

colnames(crime) <- column.names

# Add crimes per person as column
crimes_per_person <- crime$`Total Serious Crimes`/crime$`Total Pop.`
crime <- as.data.frame(cbind(crime, crimes_per_person))

# Add hospital beds per person as column
beds_per_person <- crime$`Num of Hospital Beds`/crime$`Total Pop.`
crime <- as.data.frame(cbind(crime, beds_per_person))

crime$`Geographic Region` <- as.factor(crime$`Geographic Region`) #Ensure our outcome variable is treated as a factor variable
with(crime, levels(`Geographic Region`))

head(crime)
```
```{r}
# Plot crime per person
crime_pop <- crime[order(crime$crimes_per_person), ]

barplot(crime_pop$crimes_per_person, names.arg = crime_pop$`County Name`, xlab = "County", ylab = "Crimes Per Person", main = "Crimes Per Person for each County")
```
Now we can see we no longer need to log-transform total crime and population


As found in our ANOVA tests, the following predictors have significant or near-significant differences between regions, at an alpha-level of 0.05. So, we can use these in our model.

<b> Significant </b>
* Land area: larger in West 
* Number of hospital beds per capita: lower in west, makes sense
* Number of serious crimes per capita: higher in south, Lowest in NE 
* Percent high school graduates: lowest in south
* Percent below poverty level: highest in south and west, lowest in NE
* Per capita income: highest in NE 
* Personal income: â€œ
<b> Almost significant </b>
* Percent unemployment

Next, lets split the data into training and testing, and 80/20 split.

```{r}
# Credit to https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function
train_index <- sample.int(n = nrow(crime), size = floor(.80*nrow(crime)), replace = F)
train_crime <- crime[train_index, ]
test_crime  <- crime[-train_index, ]

print(dim(train_crime))
print(dim(test_crime))
```

Now let's fit our Logistic Regression Model, log-transforming the variables that are exponentially-distributed.

```{r}
mod0 <- multinom(`Geographic Region` ~ log(`Land Area`) + beds_per_person + crimes_per_person + `Percent High School Graduates` + `Percent Below Poverty Level` + `Per Capita Income` + `Percent Unemployment`, data = train_crime)
#summary(mod0)
coefs <- as.data.frame(summary(mod0)$coefficients)
formattable(coefs, align = c("l", rep("r", NCOL(coefs) - 1)))
```


Let's see if we can judge the accuracy of this model on the training and testing datasets

```{r}
# Credit to https://datasciencebeginners.com/2018/12/20/multinomial-logistic-regression-using-r/

# Predicting the values for train dataset
train_crime$predicted <- predict(mod0, newdata = train_crime, "class")

# Building classification table
ctable_train <- table(train_crime$`Geographic Region`, train_crime$predicted)

# Calculating accuracy - sum of diagonal elements divided by total obs
train_acc <- round((sum(diag(ctable_train))/sum(ctable_train))*100,4)


# Predicting the values for test dataset
test_crime$predicted <- predict(mod0, newdata = test_crime, "class")

# Building classification table
ctable_test <- table(test_crime$`Geographic Region`, test_crime$predicted)

# Calculating accuracy - sum of diagonal elements divided by total obs
test_acc <- round((sum(diag(ctable_test))/sum(ctable_test))*100,4)

print(paste("Training Accuracy: ", train_acc, "%", sep=""))
print(paste("Testing Accuracy: ", test_acc, "%", sep=""))
```

While our accuracy is not incredibly high, we are performing much better than random, which with 4 categories would be 25%. Let's see which of our coefficients are significant.

```{r}
# Credit to https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/

z <- summary(mod0)$coefficients/summary(mod0)$standard.errors
p <- (1 - pnorm(abs(z), 0, 1)) * 2 #testing if B_i = 0 vs. B_i /= 0
p
```



We can see that there is only one coefficients that is not significant at an alpha-level of 0.05: per capita income for level 4. Since these coefficients are in comparison to our baseline level of the Northeast, then this tells us that per capita income does not differ significantly between the west and the northeast in our data according to our model.

Last thing we should do is view the exponentiated coefficients, and see what we can interpret from them:

```{r}
exp_coefs <- exp(summary(mod0)$coefficients)
exp_coefs
```

This is very interesting to see. Although many of our coefficients were significant, many of there effects are extremely close to 1. As a reminder, the interpretation of a given coefficient Blj is that assuming that a region is either NE or region l, if we increase x_j by 1 and hold all other features fixed, the odds that the outcome is region j increases by a multiplicative factor of B_lj. Therefore, if B_lj is close to 1, the odds will not change by a large amount in response to a change in x_j.

From this, we see that for Percent Unemployment and Per Capita Income, Land Area, Percent High School Graduates, Percent Below Poverty Level, these variables seem nearly inconsequential. But crimes per person and beds_per_person has tens of orders of magnitude more importance than all of the other variables, to the extent that the odds seem entirely determined by this feature.


Let's do some Likelihood ratio tests to gauge the collective relevance of certain variables in our model. First, let's test a model only using crimes_per_person and beds_per_person vs. using all other variables
```{r}
mod_red <- multinom(`Geographic Region` ~ crimes_per_person + beds_per_person, data = train_crime)
summary(mod_red)

#LRT
anova(mod_red, mod0, test="Chisq")
```
We can see that the full model, according to our test, better fits our data at any reasonable alpha-level. Now, let's try testing if per capita income has a coefficient of zero in our model.

```{r}
mod_red <- multinom(`Geographic Region` ~ log(`Land Area`) + crimes_per_person + beds_per_person + `Percent High School Graduates` + `Percent Below Poverty Level` + `Percent Unemployment`, data = train_crime)
summary(mod_red)

#LRT
anova(mod_red, mod0, test="Chisq")
```
With an alpha-level of 0.05, we reject the null hypothesis that per capita income has a coefficient of zero in our model. So, we may keep all of these variables in our model, although some seem to add little relative weight to the model decision.









